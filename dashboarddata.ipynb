{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff36fc7-3ccc-48d3-b258-cf3347ff57a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and test data\n",
      "Generating predictions and calculating metrics\n",
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.9488\n",
      "Precision: 0.9253\n",
      "Recall: 0.9417\n",
      "F1_Score: 0.9335\n",
      "ROC_AUC: 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SHAP explainability analysis...\n",
      "SHAP analysis completed on 300 samples.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    classification_report\n",
    ")\n",
    "import json\n",
    "import os\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\kamog\\Downloads\\final_model_Random Forest_20251106_223359.joblib\"\n",
    "TEST_DATA_PATH = r\"C:\\Users\\kamog\\Downloads\\dataset\\test_data2.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\kamog\\Downloads\\dataset\\dashboard_data\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading model and test data\")\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "TARGET_COL = \"Fraud\"\n",
    "\n",
    "X_test = test_data.drop(columns=[TARGET_COL])\n",
    "y_test = test_data[TARGET_COL]\n",
    "\n",
    "print(\"Generating predictions and calculating metrics\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "    \"Precision\": precision_score(y_test, y_pred),\n",
    "    \"Recall\": recall_score(y_test, y_pred),\n",
    "    \"F1_Score\": f1_score(y_test, y_pred),\n",
    "    \"ROC_AUC\": roc_auc_score(y_test, y_pred_proba)\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "with open(os.path.join(OUTPUT_DIR, \"model_metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(\"\\nModel Performance:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "pd.DataFrame(report).transpose().to_csv(os.path.join(OUTPUT_DIR, \"classification_report.csv\"))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[\"Actual: No Fraud\", \"Actual: Fraud\"],\n",
    "                     columns=[\"Predicted: No Fraud\", \"Predicted: Fraud\"])\n",
    "cm_df.to_csv(os.path.join(OUTPUT_DIR, \"confusion_matrix.csv\"))\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {metrics['ROC_AUC']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"roc_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(recall, precision, color=\"purple\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"precision_recall_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "feature_importances.to_csv(os.path.join(OUTPUT_DIR, \"feature_importance.csv\"), index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=feature_importances.head(15), x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 15 Most Important Features\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"feature_importance.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Running SHAP explainability analysis...\")\n",
    "\n",
    "# Take a small representative sample\n",
    "SAMPLE_SIZE = min(300, len(X_test))\n",
    "X_sample = X_test.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "explainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Compute mean absolute SHAP for summary\n",
    "mean_abs_shap = np.abs(shap_values[1]).mean(axis=0)\n",
    "shap_summary = pd.DataFrame({\n",
    "    \"Feature\": X_sample.columns,\n",
    "    \"Mean_Abs_SHAP\": mean_abs_shap\n",
    "}).sort_values(by=\"Mean_Abs_SHAP\", ascending=False)\n",
    "\n",
    "shap_summary.to_csv(os.path.join(OUTPUT_DIR, \"shap_summary.csv\"), index=False)\n",
    "\n",
    "# Plots\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values[1], X_sample, show=False, plot_size=(10, 6))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"shap_summary_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values[1], X_sample, show=False, plot_type=\"bar\", plot_size=(10, 6))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"shap_bar_plot.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"SHAP analysis completed on {SAMPLE_SIZE} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd176b29-d2c7-4cca-8abb-78fc3cf99f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and test data...\n",
      "Generating predictions...\n",
      "predictions.csv saved in 'C:\\Users\\kamog\\Downloads\\dataset\\dashboard_data/' — 55822 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\kamog\\Downloads\\dataset\\final_model_Random Forest_20251106_223359.joblib\"\n",
    "TEST_DATA_PATH = r\"C:\\Users\\kamog\\Downloads\\dataset\\test_data2.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\kamog\\Downloads\\dataset\\dashboard_data\"\n",
    "TARGET_COL = \"Fraud\" \n",
    "\n",
    "print(\"Loading model and test data...\")\n",
    "model = joblib.load(MODEL_PATH)\n",
    "df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "X_test = df.drop(columns=[TARGET_COL])\n",
    "y_test = df[TARGET_COL]\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": y_pred,\n",
    "    \"Fraud_Probability\": y_pred_proba\n",
    "})\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"), index=False)\n",
    "\n",
    "print(f\"predictions.csv saved in '{OUTPUT_DIR}/' — {len(predictions_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83efc5-371b-4f63-9dfc-d09c8ee533c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
